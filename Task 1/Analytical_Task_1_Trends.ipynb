{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dfc = sqlContext.read.load('file:////home/cloudera/Downloads/Q2_INPUT.csv', \n",
    "                          format='com.databricks.spark.csv', \n",
    "                          header='true', \n",
    "                          inferSchema='true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#dfc.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/spark/python/pyspark/sql/dataframe.py:142: UserWarning: Use registerTempTable instead of registerAsTable.\n",
      "  warnings.warn(\"Use registerTempTable instead of registerAsTable.\")\n"
     ]
    }
   ],
   "source": [
    "dfc.registerAsTable('train_table')\n",
    "df1 = sqlContext.sql('SELECT HashTag, (MAX(Timestamp) - MIN(Timestamp))/60 AS Life_Time FROM train_table GROUP BY HashTag')\n",
    "df1.registerAsTable('train_table7')\n",
    "df1 = sqlContext.sql('SELECT * FROM train_table7 WHERE Life_Time>0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/spark/python/pyspark/sql/dataframe.py:142: UserWarning: Use registerTempTable instead of registerAsTable.\n",
      "  warnings.warn(\"Use registerTempTable instead of registerAsTable.\")\n"
     ]
    }
   ],
   "source": [
    "dfc.registerAsTable('train_table')\n",
    "df2 = sqlContext.sql('SELECT HashTag, COUNT(*) AS Occurance FROM train_table GROUP BY HashTag ORDER BY Occurance DESC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/spark/python/pyspark/sql/dataframe.py:142: UserWarning: Use registerTempTable instead of registerAsTable.\n",
      "  warnings.warn(\"Use registerTempTable instead of registerAsTable.\")\n"
     ]
    }
   ],
   "source": [
    "df2.registerAsTable('train_table12')\n",
    "df1.registerAsTable('train_table123')\n",
    "df3 = sqlContext.sql('SELECT train_table12.HashTag, Life_Time, Occurance, ROUND(Life_Time / Occurance, 5) AS RATE_OF_OCCURANCE FROM train_table12 JOIN train_table123 ON train_table12.HashTag = train_table123.HashTag WHERE Life_Time > 600 ORDER BY Occurance DESC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#df3.coalesce(1).write.format(\"com.databricks.spark.csv\").option(\"header\", \"true\").save(\"OUTPUT_Query2_12.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------------+---------+-----------------+\n",
      "|            HashTag|         Life_Time|Occurance|RATE_OF_OCCURANCE|\n",
      "+-------------------+------------------+---------+-----------------+\n",
      "|                  d| 27730.00739300251|      305|         90.91806|\n",
      "|                hay|27369.995702052118|      274|          99.8905|\n",
      "|   thursdaythoughts| 20999.99990005096|      263|         79.84791|\n",
      "|         felizlunes| 20999.98875033458|      254|         82.67712|\n",
      "|        felizmartes|20969.996698534487|      247|         84.89877|\n",
      "|     felizmiercoles|20929.996098117033|      239|         87.57321|\n",
      "|        felizsabado| 20949.99050734838|      234|         89.52987|\n",
      "|                 be|23739.990611600875|      215|        110.41856|\n",
      "|                  s|26839.999546500047|      209|        128.42105|\n",
      "|    flashbackfriday|30519.999298715593|      198|        154.14141|\n",
      "| quintadetremurasdv|20860.012515799204|      192|         108.6459|\n",
      "|segundadetremurasdv|20919.999886834623|      186|        112.47312|\n",
      "|        buenviernes| 30699.99604743322|      180|        170.55553|\n",
      "|         felizfinde| 30769.99606708288|      178|        172.86515|\n",
      "|       felizdomingo|  20939.9921337684|      174|        120.34478|\n",
      "|                ter| 20819.99214179913|      171|        121.75434|\n",
      "| sabadodetremurasdv|20570.003051217398|      167|        123.17367|\n",
      "|        felizjueves|10900.021471480528|      164|         66.46355|\n",
      "| quartadetremurasdv| 20969.99982051452|      163|        128.65031|\n",
      "|  tuesdaymotivation|20339.995489350953|      150|        135.59997|\n",
      "+-------------------+------------------+---------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3.show()\n",
    "#df.write.format(\"com.stratio.datasource.mongodb\").mode('append').options(host=mongodb, database=dbname, collection=colname, idField='_id', splitKey='_id').save()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
